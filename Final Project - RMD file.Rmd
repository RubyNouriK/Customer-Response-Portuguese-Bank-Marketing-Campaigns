---
title: "Predicting Customer Response in A Portuguese Bank Marketing Campaigns"
author:
- Daniela Mañozca Cruz (30262558)
- Luisa Alejandra Sierra Guerra (30261956)
- Ruby Nouri Kermani (30261323)
output:
  html_document:
    df_print: paged
  word_document: default
  html_notebook: default
  pdf_document: 
    toc: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  cache = FALSE,
  dev = "cairo_pdf",
  fig.path = "figures/",
  fig.align = 'center',
  out.width = '90%'
)
```



```{r, warning=FALSE, message=FALSE, echo=FALSE}
suppressWarnings({
  library(lubridate)
  library(mctest)
  library(olsrr)
  library(ggplot2)
  library(lmtest)
  library(tibble)
  library(tidyr)
  library(kableExtra)
  library(scales)
  library(stringr)
  library(MASS)
  library(car)
  library(knitr)
  library(sampling)
  library(survey)
  library(mlbench)
  library(dplyr)
  library(tree)
  library(ISLR)
  library(AppliedPredictiveModeling)
  library(rpart)
  library(rpart.plot)
  library(corrplot)
  library(caret)
  library(QuantPsyc)
  library(klaR)
  library(kableExtra)
})
set.seed(2024)
```

\newpage

# Introduction

In today’s highly competitive marketplace, companies face relentless pressure to stand out and achieve measurable results. As competition intensifies, organizations are investing more than ever in marketing campaigns to promote new products and services. However, it is essential to assess whether these campaigns effectively meet their intended goals.

Within the banking industry, term deposits represent a critical component of the service portfolio, directly impacting financial stability and long-term planning. To promote this product, banks often rely on direct marketing strategies, particularly through telephone-based outreach. The primary objective is to convert contacts into term deposit subscriptions.

The dataset under analysis provides a comprehensive overview of a direct marketing campaign conducted by a Portuguese banking institution. It includes detailed information on client demographics, macroeconomic and social indicators. Beyond individual profiles, the dataset also captures strategic elements of the campaign (such as timing, prior contact history). Crucially, the dataset contains the final outcome variable indicating whether or not the client subscribed to a term deposit. 

Using the information from the dataset, the objective of this analysis is to develop a predictive model that estimates the likelihood of subscription. Understanding these drivers is not only a matter of campaign performance evaluation, it is a strategic necessity. By leveraging insights, financial institutions can refine their targeting strategies, enhance resource allocation, and ultimately increase the efficiency and effectiveness of their marketing operations.

# Data Sourcing and Justification

The dataset used for this project is licensed under a Creative Commons Attribution 4.0 International (CC By 4.0) license. The dataset can be found on this website: https://www.kaggle.com/datasets/henriqueyamahata/bank-marketing 

It is related to tele-marketing campaigns of a Portuguese banking institution. The classification goal is to predict if the client will subscribe to a term deposit (binary target variable). The dataset contains more than $41000$ records of $21$ variables.



**Table 1.** 
*Description of Variables in the Dataset*

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tibble)
library(kableExtra)


data_dictionary <- tribble(
  ~Variable_Name,      ~Description,                                                        ~Type,
  "age",               "Client age in years",                                               "Numeric",
  "job",               "Client main job",                                                   "Categorical",
  "marital",           "Client marital status",                                             "Categorical",
  "education",         "Client education level",                                            "Categorical",
  "default",           "Has the client credit in default?",                                 "Categorical",
  "housing",           "Does the client have a housing loan?",                              "Categorical",
  "loan",              "Does the client have a personal loan?",                             "Categorical",
  "contact",           "Contact communication type",                                        "Categorical",
  "month",             "Last contact month",                                                "Categorical",
  "day_of_week",       "Last contact day of the week",                                      "Categorical",
  "duration",          "Last contact duration in seconds",                                  "Numeric",
  "campaign",          "Contacts performed during this campaign",                           "Numeric",
  "pdays",             "Days since last contact in previous campaign",                      "Numeric",
  "previous",          "Contacts performed before this campaign",                           "Numeric",
  "poutcome",          "Outcome of the previous campaign",                                  "Categorical",
  "emp.var.rate",      "Employment variation rate",                                         "Numeric",
  "cons.price.idx",    "Consumer price index",                                              "Numeric",
  "cons.conf.idx",     "Consumer confidence index",                                         "Numeric",
  "euribor3m",         "3 month Euro Interbank Offered Rate",                               "Numeric",
  "nr.employed",       "Number of employees",                                               "Numeric",
  "y",                 "Client subscribed to a term deposit?",                              "Categorical (Binary)"
)

data_dictionary %>%
  kbl(booktabs = TRUE, longtable = TRUE) %>%
  kable_styling(latex_options = c("striped", "scale_down"), full_width = FALSE)

```


# Objective and guiding questions 

Based on the dataset, the objective of this project is to develop and evaluate predictive models to determine whether a client will subscribe to a term deposit based on various economic and campaign-related attributes. The goal is to identify key factors that influence customer decisions and improve the efficiency of future marketing campaigns. 

# Guiding questions

**1.** How do demographic attributes influence customer subscription behavior?

**2.** What is the relationship between economic indicators (e.g., employment variation rate, number of employees, EURIBOR) and customer subscription response?

**3.** How do ongoing campaign variables influence customers' willingness to subscribe?

**4.** How do previous campaign outcomes (**‘poutcome’** column) influence the likelihood of subscription to a term deposit?


```{r, warning=FALSE}
data <- read.csv("bank-full.csv")
```

# Data Cleaning

The dataset was initially explored to identify the variable names, examine its dimensions, and assess the proportion of observations based on whether or not the client subscribed to a term deposit.


**Table 2.** 
*Overview of the dataset structure, only  5 columns*

```{r, echo=FALSE, warning=FALSE}
head(data[, 1:5]) %>% 
  kbl(booktabs = TRUE, longtable = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down"), full_width = FALSE)
```


**Table 3.** 
*Dimension of the dataset*

```{r, echo=FALSE, warning=FALSE}
dims <- dim(data)

dim_table <- tibble::tibble(
  Measure = c("Number of Rows", "Number of Columns"),
  Value = dims
)

dim_table %>%
  kbl() %>%
  kable_styling(latex_options = c("hold_position"), full_width = FALSE)
```


## Missing Values

An inspection was performed to know the number of missing values per column: 

```{r}
missing_table <- data %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(cols = everything(),
               names_to = "Variable",
               values_to = "Missing_Percentage") %>%
  arrange(desc(Missing_Percentage))
```


**Table 4.** 
*Missing values in the dataset*

```{r, echo=FALSE, warning=FALSE}
missing_table %>%
  kbl(col.names = c("Variable", "Missing (%)"), digits = 2, booktabs = TRUE, longtable = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = FALSE)
```

The dataset does not contain missing values in the traditional sense; however, it was observed that some variables used the category "unknown" to indicate the absence of information. Therefore, these "unknown" entries are treated as missing values. This issue was causing problems during modeling, especially because some variables were binary (yes/no) while others, such as job categories, included these "unknown" values. Consequently, the decision was made to remove any row containing "unknown" in any variable.

```{r, warning=FALSE}
# Code to delete rows containing “unknown” in any column
data_clean <- data[!apply(data == "unknown", 1, any), ]
data <- data_clean
```


## Outliers 

Outliers were examined using the Mahalanobis distance method because it effectively identifies multivariate outliers by considering the combined variability and correlation among predictor variables.

```{r, warning=FALSE}
numeric_cols <- sapply(data, is.numeric)
x_data <- data[, numeric_cols]
x_data <- na.omit(x_data)

mahal_dist <- mahalanobis(x_data,
                          center = colMeans(x_data),
                          cov = cov(x_data))

threshold <- qchisq(0.999, df = ncol(x_data))

mahal_table <- tibble(Observation = 1:nrow(x_data),
                      Mahalanobis_Distance = mahal_dist,
                      Outlier = mahal_dist > threshold)

total_obs <- nrow(x_data)
num_outliers <- sum(mahal_table$Outlier)
```

**Table 5.** 
*Outliers*

```{r, echo=FALSE, warning=FALSE}
summary_table <- data.frame(
  Description = c("Total observations", "Number of Mahalanobis outliers"),
  Count = c(total_obs, num_outliers)
)

kable(summary_table)
```


The $2188$ observations identified as outliers represent around $7$% of the dataset; therefore, it was decided to remove these observations.


```{r, warning=FALSE}
# Code to eliminate Outliers
numeric_data <- data %>%
  filter(complete.cases(dplyr::select(., where(is.numeric))))

x_data <- dplyr::select(numeric_data, where(is.numeric))

x_data <- x_data[, apply(x_data, 2, function(col) var(col) != 0)]

mahal_dist <- mahalanobis(x_data, colMeans(x_data), cov(x_data))
threshold <- qchisq(0.999, df = ncol(x_data))
outliers <- mahal_dist > threshold

clean_data <- numeric_data[!outliers, ]

# Removing pdays
clean_data <- clean_data %>%
  dplyr::select(-pdays)

data <- clean_data
```

When this elimination was performed, one variable became constant, so it was necessary to eliminate this column **(pdays)**.


# Exploratory Data Analysis

## Marketing Campaign Performance Overview

**Figure 1.** 
*Distribution of Subscription Outcomes in the Dataset*

```{r subscription-barplot, fig.width=5, fig.height=3, echo=FALSE, warning=FALSE}
prop_df <- data %>%
  count(y) %>%
  mutate(percentage = n / sum(n))

ggplot(prop_df, aes(x = y, y = percentage, fill = y)) +
  geom_bar(stat = "identity", width = 0.6) +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)), vjust = 1.5, color = "white",size = 4)+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_fill_manual(values = c("no" = "firebrick", "yes" = "darkgreen")) +
  labs(
    title = "Subscription to Term Deposit",
    x = "Subscription Status",
    y = "Percentage"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

```


The success rate of this campaign was notably low, with $90$% of calls resulting in a rejection. This outcome is quite typical in telemarketing campaigns, where the probability of closing a sale through a single cold call is inherently low. This is primarily because cold calling involves reaching out to prospects without prior engagement or interest, which often leads to a higher rate of negative responses. 

\newpage

## Targeting Optimization (Demographic Attributes)

**Figure 2.** 
*Age Profile by Term Deposit Subscription Status*

```{r age-barplot, fig.width=5, fig.height=3, echo=FALSE, warning=FALSE}
data$y <- factor(data$y, levels = c("no", "yes"))

ggplot(data, aes(x = age, fill = y)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  scale_fill_manual(values = c("no" = "red", "yes" = "green")) +
  labs(
    title = "Age Profile by Term Deposit Subscription Status",
    x = "Age",
    y = "Count",
    fill = "Subscription Status"
  ) +
  theme_minimal()

```


The chart reveals a consistent level of subscriptions across individuals aged $20$ to $50$+, with a clear concentration of subscriber activity peaking between ages $28$ and $35$, suggesting a key demographic window for engagement.


**Figure 3.** 
*Subscription Rates by Education Level*

```{r education-barplot, fig.width=5, fig.height=3, echo=FALSE, warning=FALSE}
education_levels <- c(
  "unknown",
  "illiterate",
  "basic.4y",
  "basic.6y",
  "basic.9y",
  "high.school",
  "professional.course",
  "university.degree"
)

subscription_education <- data %>%
  mutate(education = factor(education, levels = education_levels)) %>% 
  group_by(education, y) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(education) %>%
  mutate(proportion = count / sum(count))

ggplot(subscription_education, aes(x = education, y = proportion, fill = y)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("no" = "firebrick", "yes" = "darkgreen")) +
  scale_y_continuous(labels = percent_format()) +
  labs(
    title = "Subscription Rates by Education Level",
    x = "Education Level",
    y = "Proportion",
    fill = "Subscription Satus"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


The graph illustrates subscription distribution by education level, revealing that illiterate individuals have the highest acceptance rate at nearly $25$%. This may seem surprising but is understandable, as verbal communication likely makes the service more accessible to them compared to other channels they might struggle to navigate. Conversely, those with a university degree follow as the second-highest group, exceeding $20$% adoption.

On the other end, the segment with the lowest subscription rate consists of individuals who left school before $9$th grade, reaching only $13$%. This suggests that while basic education may increase self-sufficiency in accessing services, it doesn’t necessarily correlate with higher subscription rates compared to more advanced education levels or purely oral-dependent demographics.

## Influence of Campaign Variables

**Figure 4.** 
*Temporal Distribution of Campaign Contacts by Subscription Status*

```{r, echo=FALSE}
# Prepare data
month_levels <- c("mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")

monthly_subscription <- data %>%
  mutate(month = factor(month, levels = month_levels)) %>%
  group_by(month, y) %>%
  summarise(num_clients = n(), .groups = "drop") %>%
  pivot_wider(names_from = y, values_from = num_clients, values_fill = 0)

# Rescale "yes" line to fit better
max_no <- max(monthly_subscription$no)
max_yes <- max(monthly_subscription$yes)
scale_factor <- max_no / max_yes

monthly_subscription <- monthly_subscription %>%
  mutate(yes_scaled = yes * scale_factor)

# Plot
ggplot(monthly_subscription, aes(x = month)) +
  geom_line(aes(y = no, color = "No"), size = 1.2, group = 1) +
  geom_point(aes(y = no, color = "No"), size = 2) +
  geom_line(aes(y = yes_scaled, color = "Yes"), size = 1.2, group = 1, linetype = "dashed") +
  geom_point(aes(y = yes_scaled, color = "Yes"), size = 2) +
  scale_y_continuous(
    name = "Clients (Not Subscribed)",
    sec.axis = sec_axis(~ . / scale_factor, name = "Clients (Subscribed)")
  ) +
  scale_color_manual(values = c("No" = "firebrick", "Yes" = "darkgreen"))

```

This dual-axis line chart presents the monthly distribution of campaign contacts by subscription outcome. The left y-axis displays the number of clients who did not subscribe, while the right y-axis shows those who did, with the latter re-scaled to allow for direct visual comparison. This approach was selected to account for the over $500$% difference between both groups, enabling a more nuanced interpretation of the campaign's success rate across time.

The data reveal that May stands out as the month with the highest campaign activity, surpassing $15000$ client contacts. This spike likely reflects the execution of major biannual marketing efforts. Notably, May also recorded the highest absolute number of subscriptions, despite a corresponding peak in rejections.

A secondary peak is observed in July and August, with over $5000$ non-subscribers each month. However, these months also achieved the second-highest subscription levels, suggesting a disproportionately high conversion rate relative to contact volume. This pattern indicates potentially more receptive audiences or improved campaign efficiency during mid-year periods, warranting further investigation into seasonal or contextual factors influencing consumer decisions.

Finally, November marked a minor increase in contact activity, with fewer than $5000$ rejections. Although subscription levels during this month were moderate, they contribute to a recurring pattern of campaign intensity tapering off toward the end of the year.


**Figure 5.** 
*Effect of Contact Duration on Term Deposit Subscription* 

```{r contrarct-barplot, fig.width=5, fig.height=3, echo=FALSE, warning=FALSE}
ggplot(data, aes(x = y, y = duration, fill = y)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("no" = "firebrick", "yes" = "darkgreen")) +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Term Deposit Subscription by call Duration",
    x = "Subscription Status",
    y = "Contact Duration (seconds)",
    fill = "Subscription"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

```


The graph demonstrates how engagement improves with longer call duration, revealing that calls lasting over $500$ seconds are critical for securing a **"YES"** to the service subscription. In contrast, rejected calls (**"NO"**) are significantly shorter, averaging under $250$ seconds. This trend is understandable given the nature of financial services — customers tend to be more cautious with banking-related offerings due to the perceived risks and need for clarity. As a result, call duration plays a vital role in building engagement, as it allows sales agents to establish trust, address concerns, and effectively communicate the value of the service, ultimately increasing acceptance rates.

The data underscores the importance of investing in longer, quality interactions rather than rushed pitches. A well-structured conversation that educates and reassures the customer leads to higher conversion rates, while shorter calls often fail to overcome initial skepticism. This insight suggests optimizing sales strategies to prioritize meaningful dialogue over call volume efficiency.



# Logistic Regression 

## Variance Inflation Factor

The categorical variables were first coded and converted into factors. 

```{r, warning=FALSE}
# The response variable 'y' was converted into a factor with specified levels
data$y <- factor(data$y, levels = c("no", "yes"))

# All categorical variables were converted to factors
categorical_vars <- c("job", "marital", "education", "default", "housing", "loan",
                      "contact", "month", "day_of_week", "poutcome")

data[categorical_vars] <- lapply(data[categorical_vars], as.factor)

```

The Variance Inflation Factor (VIF) was evaluated using a model that excluded the **“loan”** variable, as its inclusion led to a perfect multicollinearity error.

**Note:**

The following lines are provided as an illustrative example only. When attempting to include all predictor variables simultaneously in the logistic regression model, a perfect multicollinearity error occurs due to redundant or highly correlated predictors. Although the model runs and produces output in the R environment, it fails during the knit process when rendering to PDF format. Therefore, these lines have been commented out to avoid compilation errors:

```{r}
#LogModelFull <- glm(y~ ., data=data, family = binomial)
#vif(LogModelFull)
```


The results show that the variables **poutcome, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, and nr.employed** exhibit VIF values exceeding the commonly accepted threshold of $5$. Consequently, variable elimination was necessary to mitigate multicollinearity in the model.

The VIF results are presented in the following table. Due to the presence of perfect multicollinearity among several predictors, the values were extracted and compiled manually to ensure compatibility with the document rendering process.

**Table 6.** 
*Variance Inflation Factor (VIF)*

```{r, echo=FALSE, warning=FALSE}
vif_table <- data.frame(
  Variable = c("age", "job", "marital", "education", "default", "housing", "contact",
               "month", "day_of_week", "duration", "campaign", "previous",
               "poutcome", "emp.var.rate", "cons.price.idx", "cons.conf.idx",
               "euribor3m", "nr.employed"),
  GVIF = c(2.312897, 5.832002, 1.465762, 3.197933, 1.142854, 1.014135, 2.318299,
           63.049855, 1.066399, 1.243535, 1.052431, 4.474373,
           24.228747, 142.232401, 68.108673, 5.333698, 135.037594, 172.009860),
  Df = c(1, 11, 3, 7, 2, 2, 1, 9, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1)
)

vif_table$`GVIF^(1/(2*Df))` <- with(vif_table, round(GVIF^(1/(2 * Df)), 6))

kable(vif_table, align = "lrrr")

```

Based on the results, it was necessary to exclude the variables with a Variance Inflation Factor (VIF) greater than $5$. As a result, only the following variables were retained in the model:


```{r, echo=FALSE}
# Model without variables that have multicollinearity code
LogModel0 <- glm(y ~ age + job +  marital + education + default + housing + 
                  contact + month + day_of_week + duration + campaign + + previous +
                  emp.var.rate + cons.conf.idx, data = data, family = binomial)

n <- vif(LogModel0)
```

**Table 7.** 
*Variance Inflation Factor (VIF) Final*

```{r, echo=FALSE, warning=FALSE}
# Crear tabla actualizada de VIF
vif_table <- data.frame(
  Variable = c("age", "job", "marital", "education", "default", "housing", "contact",
               "month", "day_of_week", "duration", "campaign", "previous",
               "emp.var.rate", "cons.conf.idx"),
  GVIF = c(2.054999, 5.190247, 1.395131, 3.072495, 1.000001, 1.010704, 1.706823,
           6.895378, 1.061853, 1.267266, 1.051676, 1.091259,
           2.273321, 3.513584),
  Df = c(1, 10, 2, 6, 1, 1, 1, 9, 4, 1, 1, 1, 1, 1)
)

vif_table$`GVIF^(1/(2*Df))` <- with(vif_table, round(GVIF^(1/(2 * Df)), 6))

library(knitr)
kable(vif_table, align = "lrrr")
```



## Likelihood - Ratio Test 

To assess variable significance, we first applied the Wald Z-test through a logistic regression model including a comprehensive set of predictors:

```{r}
LogModelFull<- glm(y ~ age + job +  marital + education + default + housing + 
                  contact + month + day_of_week + duration + campaign + previous +
                  emp.var.rate + cons.conf.idx, data = data, family = binomial)
summary(LogModelFull)
```


**Table 8.** 
*Summary of Variable Significance Based on Wald z-test*

```{r, echo=FALSE, warning=FALSE}
LogModelFull<- glm(y ~ age + job +  marital + education + default + housing + 
                  contact + month + day_of_week + duration + campaign + + previous +
                  emp.var.rate + cons.conf.idx, data = data, family = binomial)

all_vars <- c("age", "job", "marital", "education", "default", "housing", 
              "contact", "month", "day_of_week", "duration", "campaign", 
              "previous", "emp.var.rate", "cons.conf.idx")

significant_vars <- c("job", "education", "month", "day_of_week", 
                      "duration", "campaign", "emp.var.rate", "cons.conf.idx")

final_table <- data.frame(
  Variable = all_vars,
  Significant = ifelse(all_vars %in% significant_vars, "Yes", "No")
)

library(knitr)
kable(final_table)
```

Since several variables were found to be non-significant, we employed a stepwise selection procedure using both forward and backward directions to retain only significant predictors:

```{r, echo=FALSE, warning=FALSE}
reduced_model <- step(LogModelFull, direction = "both", trace = FALSE)
summary(reduced_model)
```


As a result, five variables were excluded from the model: age, marital, default, housing, and contact.
We now have two models:

- Full Model: includes $14$ predictors

- Reduced Model: includes only $9$ predictors

To evaluate whether the reduced model provides a statistically comparable fit to the full model, we conducted a Likelihood Ratio Test. This test compares the log-likelihoods of the two models. While removing predictors usually reduces model fit, the test allows us to determine if the decrease is statistically significant.

**Hypotheses:**

$$
\begin{aligned}
H_0:&\beta_{r+1}=\beta_{r+2}...=\beta_{p}=0 (\mbox{           reduced model is true})\\
H_1:&\mbox{at least one }\beta_i\neq0 (\mbox{                  larger model is true})\\\\
\mbox{The likelihood}&\mbox{ ratio statistic is}\\\\\\
\bigtriangleup G^2 &=-2logL_{reduced} -(-2logL_{larger})\\\\
\mbox{The p-value is }& p(\chi^2>\bigtriangleup G^2)
\end{aligned}
$$

```{r, warning=FALSE}
anova(LogModelFull, reduced_model, test = "Chisq")

```

The results indicate that the p-value exceeds $0.05$, suggesting there is insufficient evidence to reject the null hypothesis. Therefore, the reduced model should be preferred, as it offers a more parsimonious representation without a significant loss in explanatory power. Additionally, by excluding non-contributing predictors, the reduced model minimizes complexity and mitigates the risk of over-fitting.

A final model comparison will be performed using a validation approach to confirm these findings and assess predictive performance. 

## Validation Approach

As observed in the exploratory data analysis (EDA), the subscription success rate (i.e., instances where the individual subscribed) was only $11$%. Given the limited number of "yes" responses, a manual stratified sub-setting approach was implemented. Specifically, $80$% of the "yes" responses and $80$% of the "no" responses were assigned to the test dataset, while the remaining 20% of each group were allocated to the training dataset. This ensured that both datasets preserved the original class distribution.

```{r, warning=FALSE}
#Code to create train and test dataset  
set.seed(2024)
data$id <- 1:nrow(data)

# Separate dataset in "yes" or "no"
yes_data <- data[data$y == "yes", ]
no_data  <- data[data$y == "no",  ]

# Select the same proportion on each category 
train_yes_idx <- sample(1:nrow(yes_data), 0.8 * nrow(yes_data))
train_no_idx  <- sample(1:nrow(no_data),  0.8 * nrow(no_data))

train_data <- rbind(yes_data[train_yes_idx, ], no_data[train_no_idx, ])
test_data  <- anti_join(data, train_data, by = "id")

train_data <- train_data %>%
  dplyr::select(-id)

test_data <- test_data %>%
  dplyr::select(-id)

```

**Table 9.** 
*Records in Train and Test Datasets*

```{r, echo=FALSE, warning=FALSE}
data_summary <- data.frame(
  Dataset = c("Train", "Test"),
  Records = c(nrow(train_data), nrow(test_data))
)
kable(data_summary)
```



## Final Model GLM 

To compare the predictive performance of the full and reduced models, both were trained on the training dataset and evaluated on the test dataset using a classification threshold of $0.5$. 

**Full Model**

```{r, warning=FALSE}
LogModel <- glm(y ~ age + job +  marital + education + default + housing + 
                  contact + month + day_of_week + duration + campaign + previous +
                  emp.var.rate + cons.conf.idx, data = train_data, family = binomial)


Prob.predict <- predict(LogModel, test_data, type="response")
Predict <- rep("no", dim(test_data)[1])
Predict[Prob.predict>=0.5]="yes"
Actual <- test_data$y

```

**Table 10.** 
*Confusion Matrix for Logistic Regression - Full Model*

```{r, echo=FALSE, warning=FALSE}
kable(table(Predict, Actual))
```


```{r, warning=FALSE}
conf_matrix <- table(Predict, Actual)
misclassification_rate <- sum(Predict != Actual) / length(Actual)
print(paste("Misclassification Rate:", round(misclassification_rate, 4)))
```

The full model correctly identified $4969$ true negatives and $159$ true positives were correctly predicted. However, there were $134$ false negatives and $399$ false positives. The overall misclassification rate is $9.42$%, indicating that the model accurately classifies approximately $90.58$% of the cases. 

**Reduced Model:**

```{r}
Prob.predict <- predict(reduced_model, test_data, type="response")
Predict <- rep("no", dim(test_data)[1])
Predict[Prob.predict>=0.5]="yes"
Actual <- test_data$y

conf_matrix <- table(Predict, Actual)
misclassification_rate <- sum(Predict != Actual) / length(Actual)
```


**Table 11.** 
*Confusion Matrix for Logistic Regression - Reduced Model*

```{r, echo=FALSE, warning=FALSE}
kable(table(Predict, Actual))
print(paste("Misclassification Rate:", round(misclassification_rate, 4)))
```

The reduced model correctly predicted $4972$ true negatives and $161$ true positives were correctly predicted. However, there were $131$ false negatives and $397$ false positives. The overall misclassification rate is $9.33$%, indicating that the model accurately classifies approximately $90.67$% of the cases. 

Although the performance improvement in terms of misclassification is marginal, the reduced model is preferred due to its:

- Lower complexity, using fewer predictors

- Improved interpretability

- Reduced risk of over-fitting

Therefore, the reduced model is selected as the final model for this logistic regression analysis.


```{r}
reduced_model <- glm(y~ job + education + contact + 
                       month + day_of_week + duration+
                       campaign + emp.var.rate+ cons.conf.idx,
                     data = data, family = binomial)
#summary(reduced_model)
```

* (Dispersion parameter for binomial family taken to be $1$)

* Null deviance: $18219$  on $28299$  degrees of freedom
    
* Residual deviance: $11535$  on $28265$  degrees of freedom

* AIC: $11605$

* Number of Fisher Scoring iterations: $6$

**Table 12.** 
*Summary for Logistic Regression Final Model*

```{r, echo=FALSE}
model_results <- data.frame(
  Variable = c("(Intercept)", 
               "jobblue-collar", "jobentrepreneur", "jobhousemaid", "jobmanagement", 
               "jobretired", "jobself-employed", "jobservices", "jobstudent", "jobtechnician", 
               "jobunemployed",
               "educationbasic.6y", "educationbasic.9y", "educationhigh.school", 
               "educationilliterate", "educationprofessional.course", "educationuniversity.degree",
               "contacttelephone",
               "monthapr", "monthmay", "monthjun", "monthjul", "monthaug", "monthsep", 
               "monthoct", "monthnov", "monthdec",
               "day_of_weekmon", "day_of_weekthu", "day_of_weektue", "day_of_weekwed",
               "duration", "campaign", "emp.var.rate", "cons.conf.idx"),
  
  Estimate = c(-1.660e+00,
               -2.590e-01, -1.787e-01, 7.496e-02, -1.389e-01, 
               3.494e-01, -1.071e-01, -2.400e-01, 4.198e-01, -4.397e-02, 
               1.450e-01,
               1.702e-02, 3.508e-02, 1.077e-01, 
               1.502e+00, 1.671e-01, 2.804e-01,
               -1.492e-01,
               -1.817e+00, -2.631e+00, -1.486e+00, -1.477e+00, -1.718e+00, -2.415e+00,
               -1.512e+00, -2.536e+00, -1.537e+00,
               -4.596e-03, 8.058e-02, 1.518e-01, 2.234e-01,
               5.306e-03, -3.866e-02, -6.670e-01, 2.151e-02),
  
  Std_Error = c(3.993e-01,
                9.915e-02, 1.483e-01, 1.839e-01, 1.008e-01,
                1.197e-01, 1.352e-01, 1.044e-01, 1.374e-01, 8.421e-02,
                1.527e-01,
                1.605e-01, 1.224e-01, 1.185e-01,
                9.036e-01, 1.297e-01, 1.187e-01,
                8.181e-02,
                1.317e-01, 1.252e-01, 1.345e-01, 1.416e-01, 1.564e-01, 2.394e-01,
                2.065e-01, 1.495e-01, 2.459e-01,
                8.072e-02, 7.900e-02, 8.064e-02, 8.036e-02,
                9.627e-05, 1.548e-02, 2.134e-02, 7.717e-03),
  
  z_value = c(-4.158,
              -2.612, -1.205, 0.408, -1.377,
              2.920, -0.792, -2.299, 3.055, -0.522,
              0.950,
              0.106, 0.287, 0.908,
              1.662, 1.288, 2.362,
              -1.823,
              -13.799, -21.021, -11.049, -10.428, -10.981, -10.087,
              -7.323, -16.962, -6.252,
              -0.057, 1.020, 1.883, 2.780,
              55.110, -2.498, -31.248, 2.788),
  
  p_value = c(3.22e-05,
              0.00900, 0.22815, 0.68357, 0.16838,
              0.00350, 0.42824, 0.02153, 0.00225, 0.60158,
              0.34217,
              0.91557, 0.77447, 0.36370,
              0.09643, 0.19770, 0.01816,
              0.06829,
              2e-16, 2e-16, 2e-16, 2e-16, 2e-16, 2e-16,
              2.43e-13, 2e-16, 4.06e-10,
              0.95460, 0.30775, 0.05971, 0.00544,
              2e-16, 0.01248, 2e-16, 0.00531)
)


kable(model_results, digits = 4)

```



# Linear Discriminant Analysis (LDA)

## LDA Assumptions Validation

The criteria for LDA is the following:

### High correlation validation 

Highly correlated predictors can cause problems because LDA relies on the inverse of the covariance matrix. (Correlation matrix can be checked)

**Figure 6.** 
*Correlation Matrix*

```{r, echo=FALSE, warning=FALSE}
#Multicollinearity review
data_corr <- data %>%
  dplyr::select(-id)
cor_matrix <- cor(data_corr[sapply(data_corr, is.numeric)], use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "lower", tl.cex = 0.8)
```

The variables that show very strong correlations are:

* **euribor3m** and	**nr.employed**
* **emp.var.rate** and **euribor3m**
* **emp.var.rate** and **nr.employed**

The variable **id** also exhibit a high correlation with multiple variables, but since is an identifier, it will not be consider in the modeling. Based on the results the variables that will be excluded is **"nr.employed"** and **"euribor3m"**.

### Normal distribution within classes

LDA assumes the explanatory variables are normally distributed within each class of the response variable.We are going to use Mardia’s Test, which determines whether or not a group of variables follows a multivariate normal distribution. The null and alternative hypotheses for the test are as follows:

**H0 (null hypothesis)**: The variables follow a multivariate normal distribution.

**Ha (alternative hypothesis)**: The variables do not follow a multivariate normal distribution.

```{r, warning=FALSE}
#The dependent variable has 2 classes: Yes and No.
#The explanatory variables must be numerical
# The variable "nr.employed" and "euribor3m" are excluded

predictors<-c("age", "duration","campaign", "previous", "emp.var.rate" ,
                "cons.price.idx", "cons.conf.idx")
yes_data_numerical<-subset(yes_data, select = predictors)
no_data_numerical<-subset(no_data, select = predictors)
```

```{r, warning=FALSE}
# Mardia's multivariate normality test for Yes class
mult.norm(yes_data_numerical)$mult.test
```

Given p-value for Skewness and Kurtosis are extremely small, less than $0.05$, we reject the null hypothesis for both test, meaning the Multivariate normality assumption is not met for the **"Yes"** class. LDA assumes that the predictor variables are multivariate normally distributed within each class, since this assumption is violated the LDA boundary for classifying could be less reliable.


For performance issues, the Normality test for the **No Class** will be run only over the $10$% of the data.

```{r, warning=FALSE}
set.seed(10)
sample_size <- 0.1*nrow(no_data_numerical)
idx <- sample(1:nrow(no_data_numerical), size = sample_size)
no_data_numerical_10 <- no_data_numerical[idx, ]
```

```{r, warning=FALSE}
# Mardia's multivariate normality test for No class
mult.norm(no_data_numerical_10)$mult.test
```

Given p-value for Skewness and Kurtosis are less than $0.05$, we reject the null hypothesis for both test, meaning the Multivariate normality assumption is not met for the **"No"** class.

If the second assumption (equal covariance between classes) is not met either, we can try QDA modeling or non parametric modeling as classification tree (this does not assume normality).

### Equal variance (Homoscedasticity) for predictor variables within each class

LDA relies on Equal variance (Homoscedasticity) for predictor variables within each class. For Equality of Variances test we are going to use Levene’s test given that our data set is not meeting th normality distribution assumption.

**H0 (null hypothesis)**: The variances are equal between groups ("Yes" vs "No")

**Ha (alternative hypothesis)**: The variances are not equal between groups ("Yes" vs "No")


```{r, warning=FALSE}
library(car)

predictors_lda<-c("age", "duration","campaign", "previous", "emp.var.rate" ,
                "cons.price.idx", "cons.conf.idx", "y")
data_lda<-subset(data, select = predictors_lda)

#Test for age
leveneTest(age ~ y, data=data_lda)
#Test for duration
leveneTest(duration ~ y, data=data_lda)
```

Since p-values are very small (< $0.05$) for both variables, we reject the null hypothesis. This means the assumption of equal variances is violated for both age and duration.

```{r, warning=FALSE}
#Test for previous
leveneTest(previous ~ y, data=data_lda)

#Test for emp.var.rate
leveneTest(emp.var.rate ~ y, data=data_lda)
```

For the variables previous and **emp.var.rate**, the assumption of equal variances within classes is not met.

P-values are very small (< $0.05$) for both variables.

```{r, warning=FALSE}
#Test for cons.price.idx
leveneTest(cons.price.idx ~ y, data=data_lda)

#Test for cons.conf.idx
leveneTest(cons.conf.idx ~ y, data=data_lda)
```

For the variable **"cons.price.idx"** variances are not significantly different between the classes, so we fail to reject the Null Hypothesis, meaning the Homoscedasticity assumption is met for this variable. On the contrary, **"cons.conf.idx"** violates the assumption since the P-value in the test is very small (close to zero).

Although the assumptions are not fully met, we will proceed with modeling the dependent variable using LDA. This decision is based on LDA's practical robustness, and our goal is to compare its accuracy with other models such as QDA, which does not require equal covariance assumptions—and classification tree, which is non-parametric.

## Final Model LDA

Based on the previously explained rationale, the following LDA model was implemented:

```{r, warning=FALSE}
#Model without nr.employed, euriborm3m
set.seed(2024)
predictors_lda<-c("age", "duration","campaign", "previous", "emp.var.rate" ,
                "cons.price.idx", "cons.conf.idx", "y")

lda.fit3<-lda(y~age+duration+campaign+previous+emp.var.rate+cons.price.idx+cons.conf.idx,
              data = train_data)
lda.fit3
```

Applying the fitted model to the test set:

```{r}
actual = test_data$y
#Confusion matrix
y.pred = predict(lda.fit3, test_data)$class 
```


**Table 13.** 
*Confusion Matrix for LDA*

```{r, echo=FALSE, warning=FALSE}
kable(table(y.pred, actual)) 
```

The model correctly classified $4899$ clients as not subscribing to the term deposit (True Negatives) and $229$ clients as subscribing (True Positives). However, it also misclassified $329$ clients who did not subscribe as subscribers (False Positives), and $204$ clients who actually subscribed were missed by the model (False Negatives).

```{r, warning=FALSE}
# Misclassification Rate
conf_mat_lda <- table(Predicted = y.pred, Actual = actual)
incorrect <- sum(conf_mat_lda) - sum(diag(conf_mat_lda))
total <- sum(conf_mat_lda)
misclassification_rate <- incorrect / total
paste("Misclassification Rate:", round(misclassification_rate*100, 4), "%")
```

The misclassification rate for the final LDA model is $9.41$%.

These results suggest that the model performs quite well in identifying clients who are unlikely to subscribe, but it struggles to accurately predict those who will subscribe. This is likely due to class imbalance, where non-subscribers dominate the dataset, even when the data split was balanced before generating the dataset split. As a result of the imbalanced data, the model is biased toward the majority class, and its ability to capture positive cases (subscribers) is limited.


```{r, warning=FALSE}
train_pplot<-train_data[,!names(train_data) %in%
                          c("job","marital","education","default",
                            "housing","loan","contact",
                            "month","day_of_week", "poutcome" )]
##partimat(y~., data=train_pplot, method="lda")
```


**Figure 7.** 
*Partition Boundaries of Top Two Variable Pairs Using LDA*

```{r partimat-top2, fig.width=6, fig.height=4, warning=FALSE}
library(klaR)
library(dplyr)
library(combinat)  # for combn

# Subset numeric predictors only
vars <- c("age", "duration", "campaign",
          "previous", "emp.var.rate",
          "cons.price.idx", "cons.conf.idx")
train_pp <- train_data[, c(vars, "y")]

# Store error rates
results <- data.frame(var1 = character(), var2 = character(), error = numeric())

# Loop over all 2-variable combinations
combinations <- combn(vars, 2)

for (i in 1:ncol(combinations)) {
  pair <- combinations[, i]
  formula <- as.formula(paste("y ~", paste(pair, collapse = " + ")))
  
  # Fit model and compute apparent error
  model <- lda(formula, data = train_pp)
  pred <- predict(model)$class
  error <- mean(pred != train_pp$y)
  
  results <- rbind(results, data.frame(var1 = pair[1], var2 = pair[2], error = error))
}

# Get top 2 lowest error pairs
top2 <- results %>% arrange(error) %>% head(2)

# Plot them
for (i in 1:2) {
  v1 <- top2$var1[i]
  v2 <- top2$var2[i]
  cat(paste("Plotting: ", v1, " vs. ", v2, " | error rate = ", round(top2$error[i], 4), "\n"))
  print(partimat(as.formula(paste("y ~", v1, "+", v2)), data = train_pp, method = "lda"))
}
```


# Quadratic Discriminant Analysis (QDA)

Running QDA on all numerical variables:

```{r, warning=FALSE}
set.seed(2024)
qda_fit <- qda(y~ age + duration + campaign + previous +
                 emp.var.rate + cons.price.idx + 
                 cons.conf.idx + euribor3m + nr.employed,
               data = train_data)
qda_fit
```

**Table 14.** 
*Confusion Matrix for QDA on all numerical variables*

```{r, echo=FALSE, warning=FALSE}
actual = test_data$y
#Confusion matrix
qda_pred = predict(qda_fit, test_data)$class 
kable(table(qda_pred, actual)) 
```

The model correctly classified $4648$ clients as not subscribing to the term deposit (True Negatives) and $346$ clients as subscribing (True Positives). However, it also misclassified $212$ clients who did not subscribe as subscribers (False Positives), and $455$ clients who actually subscribed were missed by the model (False Negatives).


```{r, warning=FALSE}
# Misclassification rate
conf_mat_qda <- table(Predicted = qda_pred, Actual = actual)
incorrect <- sum(conf_mat_qda) - sum(diag(conf_mat_qda))
total <- sum(conf_mat_qda)
misclassification_rate <- incorrect / total
paste("Misclassification Rate:", round(misclassification_rate*100, 4), "%")
```

Misclassification rate for QDA in all numeric values is $11.78$%. 

There's a chance some features are redundant, and removing them might improve generalization.

```{r, warning=FALSE}
train_data_numeric <- train_data %>%
  dplyr::select(where(is.numeric), y)
```


```{r, warning=FALSE}
cor_matrix <- cor(dplyr::select(train_data_numeric, -y))
high_corr <- findCorrelation(cor_matrix, cutoff = 0.95)
names(train_data_numeric)[high_corr]
```

We will drop **'euribor3m'** because of it's high correlation and we will run the QDA model on all variables except for this column.


```{r, warning=FALSE}
qda_reduced <- qda(y~ age + duration + campaign + previous +
                     emp.var.rate + cons.price.idx +
                     cons.conf.idx + nr.employed,
                   data = train_data)
qda_reduced
```

```{r}
actual = test_data$y
#Confusion matrix
qda_pred1 = predict(qda_reduced, test_data)$class 
```

**Table 15.** 
*Confusion Matrix for QDA variables except for euribor3m*

```{r, echo=FALSE, warning=FALSE}
kable(table(qda_pred1, actual))
```

The model correctly classified $4644$ clients as not subscribing to the term deposit (True Negatives) and $340$ clients as subscribing (True Positives). However, it also misclassified $218$ clients who did not subscribe as subscribers (False Positives), and $459$ clients who actually subscribed were missed by the model (False Negatives).

```{r, warning=FALSE}
# Misclassification rate
conf_mat_qda1 <- table(Predicted = qda_pred1, Actual = actual)
incorrect <- sum(conf_mat_qda1) - sum(diag(conf_mat_qda1))
total <- sum(conf_mat_qda1)
misclassification_rate1 <- incorrect / total
paste("Misclassification Rate:", round(misclassification_rate1*100, 4), "%")
```

The misclassification rate didn't change much ($11.95$%), so we will drop **nr.employed** as well.

## Final Model QDA

```{r, warning=FALSE}
qda_reduced2 <- qda(y~ age + duration + campaign + previous
                    + emp.var.rate + cons.price.idx + cons.conf.idx,
                    data = train_data)
qda_reduced2
```
```{r}
actual = test_data$y
#Confusion matrix
qda_pred2 = predict(qda_reduced2, test_data)$class 
```


**Table 16.** 
*Confusion Matrix for QDA variables except for euribor3m and nr.employed*

```{r, echo=FALSE, warning=FALSE}
kable(table(qda_pred2, actual))
```

The model correctly classified $4695$ clients as not subscribing to the term deposit (True Negatives) and $316$ clients as subscribing (True Positives). However, it also misclassified $242$ clients who did not subscribe as subscribers (False Positives), and $408$ clients who actually subscribed were missed by the model (False Negatives).

```{r, warning=FALSE}
# Misclassification rate
conf_mat_qda2 <- table(Predicted = qda_pred2, Actual = actual)
incorrect <- sum(conf_mat_qda2) - sum(diag(conf_mat_qda2))
total <- sum(conf_mat_qda2)
misclassification_rate2 <- incorrect / total
paste("Misclassification Rate:", round(misclassification_rate2*100, 4), "%")
```

By dropping **euribor3m** and **nr.employed**, the misclassification rate reduced very little to $11.48$%.

Since **emp.var.rate** was also a high correlated column, I will drop that too to see if the model accuracy gets better or not.

```{r, warning=FALSE}
qda_reduced3 <- qda(y~age+duration+campaign+previous+cons.price.idx+cons.conf.idx,
                    data = train_data)
qda_reduced3
```
```{r}
actual = test_data$y
#Confusion matrix
qda_pred3 = predict(qda_reduced3, test_data)$class 
```


**Table 17.** 
*Confusion Matrix for QDA variables except for euribor3m and nr.employed and emp.var.rate*

```{r, echo=FALSE, warning=FALSE}
kable(table(qda_pred3, actual))
```

The model correctly classified $4764$ clients as not subscribing to the term deposit (True Negatives) and $267$ clients as subscribing (True Positives). However, it also misclassified $291$ clients who did not subscribe as subscribers (False Positives), and $339$ clients who actually subscribed were missed by the model (False Negatives).

```{r, warning=FALSE}
# Misclassification rate
conf_mat_qda3 <- table(Predicted = qda_pred3, Actual = actual)
incorrect <- sum(conf_mat_qda3) - sum(diag(conf_mat_qda3))
total <- sum(conf_mat_qda3)
misclassification_rate3 <- incorrect / total
paste("Misclassification Rate:", round(misclassification_rate3*100, 4), "%")
```

The misclassification rate reduced to $11.12$%.

This means that **euribor3m** and **nr.employed**and **emp.var.rate** are highly correlated and by removing them, we made our model quite a bit more stable.

**Figure 8.** 
*QDA Partition Plot*

```{r, echo=FALSE, warning=FALSE}
partimat(y ~ age + duration + campaign + cons.price.idx + cons.conf.idx, data=train_data, method="qda")
```



This reduction suggests that eliminating redundant features can enhance QDA performance by reducing instability in the covariance matrices without sacrificing predictive power.


# Classification Tree Model 

As seen in the previous models (LDA and QDA), the assumptions of normality and equal variances were not met. We will now proceed to model a classification tree, which is a non-parametric method and does not rely on these two assumptions. Although classification trees require the observations to be independent, this condition is satisfied in our dataset, as we are working with individual records from different bank clients, and no relationships between clients are indicated.

Additionally, classification trees do not assume linear relationships between the dependent and independent variables, which means they are good for capturing non-linear patterns, as the model is based on recursive decision rules. While trees are generally robust to multicollinearity, highly correlated predictors can lead to instability. Therefore, we excluded the two variables with the highest observed correlation in the dataset: **"nr.employed"** and **"euribor3m"** (as we did in the previous models).

The tree will be evaluated using cross-validation to determine whether pruning is appropriate. This will help mitigate the risk of over-fitting and improve the model’s generalization performance.

```{r}
#Making sure the 2 variables with high correlation are dropped "nr.employed" and "euribor3m"
train_data <- train_data %>%
  select(-nr.employed, -euribor3m)

test_data<-test_data %>%
  select(-nr.employed, -euribor3m)

#Making sure the dataset is complete withou splitting for the k fold
new_data <- rbind(train_data, test_data)

```

```{r}
#Apply a classification tree to the train part to establish relation between "y" and other variables.
tree_bank<-tree(factor(y)~., data=train_data)
summary(tree_bank)
```


**Figure 9.** 
*Full Classification Tree*

```{r classification-barplot, fig.width=5, fig.height=3, echo=FALSE, warning=FALSE}
par(mar = c(1, 1, 2, 1), cex = 1.2)
plot(tree_bank, col = "blue", lty = 5)
text(tree_bank, pretty = 0, cex = 0.6)
title("Classification Tree for y", line = 1)
```
```{r}
tree_bank_pred<-predict(tree_bank,test_data,type = "class")
```



**Table 18.** 
*Confusion Matrix for Classification Tree Model*
```{r, echo=FALSE}
kable(table(tree_bank_pred,test_data$y))
```

Perform cross-validation:

**Figure 10.** 
*Classification Error by Tree Size from Cross-Validation Pruning*

```{r nodes-barplot, fig.width=5, fig.height=3, echo=FALSE, warning=FALSE}
cv_bank<-cv.tree(tree_bank, FUN = prune.misclass) 
plot(cv_bank$size, cv_bank$dev,type="b")
```

## Final Model Classification Tree Model

Based on cross-validation pruning, the tree will be pruned to 5 terminal nodes for the final model

```{r}
prune_bank=prune.tree(tree_bank,best=5)
```

**Figure 11.** 
*Pruned Classification Tree Based on Cross-Validation*

```{r final-barplot, fig.width=5, fig.height=3, echo=FALSE, warning=FALSE}
par(mar = c(1, 1, 2, 1), cex = 1.2)
plot(prune_bank, col = "blue", lty = 5)
text(prune_bank, pretty = 0, cex = 0.6)
title("Classification Prunned Tree for y", line = 1)
```

re-calculate the misclassification rate:

```{r}
#Apply the prune tree to the test set
prune_bank_pred<-predict(prune_bank,test_data,type="class")
```


**Table 19.** 
*Confusion Matrix for the Final Classification Tree Model*
```{r}
kable(table(Predicted=prune_bank_pred,Actual=test_data$y))
```

```{r}
# Misclassification Rate
confusion_prune=table(Predicted=prune_bank_pred,Actual=test_data$y)
incorrect <- sum(confusion_prune) - sum(diag(confusion_prune))
total <- sum(confusion_prune)
misclassification_rate <- incorrect / total
paste("Misclassification Rate:", round(misclassification_rate*100, 4), "%")
```

The pruned classification tree model correctly identified 4,903 negative cases (clients who did not subscribe) and 180 positive cases (clients who did subscribe). However, it also produced 378 false positives (predicted as subscribers, but they were not) and 200 false negatives (predicted as non-subscribers, but they actually subscribed).From this confusion matrix, we can estimate the following performance metrics:

* Accuracy: $89.8$%. This represents the proportion of correctly classified observations out of all predictions.

* Precision for yes: $32.2$%. This is the proportion of correct positive predictions among all predicted positives (the true and false ones).
 
* Sensitivity for yes: $47.4$%. This indicates the model’s ability to correctly identify actual subscribers $(180/(180+200))$.

* Specificity is: $92.8$%. This is the true negative rate and reflects the model’s ability to correctly classify non-subscribers.(TN/Total Neg= $4903/(4903+378))$.

* Misclassification rate of $10.2$%. This is estimated by all the incorrectly classified over the total predicted and it is the complement of the accuracy.

Overall, the model performs well in terms of general accuracy and specificity, which is expected given the dataset’s imbalance toward non-subscribers. Although class balancing was applied before splitting the data into training and test sets, the prediction of the minority class **('yes')** remains challenging.

Only $47.4$% of actual subscribers were correctly identified, which indicates poor sensitivity. This highlights the model’s limited ability to detect clients who will subscribe, which is a crucial insight in applications where identifying positive cases is a priority.


# K-fold Cross-validation.

A $10$-fold cross-validation procedure was used to compare the performance of the Linear Discriminant Analysis and Classification Tree models.


```{r, warning=FALSE}
#10 folds partition
folds<-createFolds(factor(new_data$y), k=10)
```

**Table 20 and 21.** 
*Distribution of Class Labels in Fold 1 and Fold 10*

```{r, echo=FALSE}
library(knitr)

#Check number of Types in each fold
fold1<-new_data[folds$Fold1,]
#Check number of Types in each fold
fold10<-new_data[folds$Fold10,]

kable(table(fold1$y))
kable(table(fold10$y))
```



## Misclassification for GLM with K folds

```{r, warning=FALSE}
misclassification <- function(idx) {
  Train <- new_data[-idx, ]
  Test  <- new_data[idx, ]
  fit <- glm(y~job+education+contact+month+day_of_week+duration+campaign+emp.var.rate+cons.conf.idx,
             family = binomial,
             data = Train)
  prob <- predict(fit, newdata = Test, type = "response")
  pred <- rep("no", length(prob))
  pred[prob >= 0.5] <- "yes"
  return(1 - mean(pred == Test$y))
}

mis_rate=lapply(folds,misclassification)

cv_error_glm=mean(as.numeric(mis_rate))
paste("Misclassification Rate for glm:", round(cv_error_glm*100, 2), "%")
```







## Misclassification for LDA with K folds

```{r, warning=FALSE}
misclassification<-function(idx){
  Train<-new_data[-idx,]
  Test<-new_data[idx,]
  fit<-lda(y~ age + duration + campaign + previous + emp.var.rate + 
    cons.price.idx + cons.conf.idx, data=train_data)
  pred<-predict(fit,Test)
  return(1-mean(pred$class==Test$y))
}

#Passing the function in the folds
mis_rate=lapply(folds,misclassification)

#Average of the missclasification
cv_error_lda=mean(as.numeric(mis_rate))
paste("Misclassification Rate for lda:", round(cv_error_lda*100, 2), "%")
```


## Misclassification for QDA with K folds:

```{r, echo=FALSE, warning=FALSE}
misclassification<-function(idx){
  Train<-new_data[-idx,]
  Test<-new_data[idx,]
  fit <- qda(y ~ age + duration + campaign + previous + cons.price.idx + cons.conf.idx, data = train_data)
  pred<-predict(fit,Test)
  return(1-mean(pred$class==Test$y))
}

#Passing the function in the folds
mis_rate=lapply(folds,misclassification)

#Average of the missclasification
cv_error_lda=mean(as.numeric(mis_rate))
paste("Misclassification Rate for qda:", round(cv_error_lda*100, 2), "%")

```


## Misclassification for Classification Tree with K folds:

```{r, warning=FALSE}
misclassification<-function(idx){
  Train <- new_data[-idx, ]
  Test<- new_data[idx, ]
  fit <-prune.tree(tree_bank,best=7)
  pred  <- predict(fit, Test, type = "class")
  return(1 - mean(pred == Test$y))
}

#Passing the function in the folds
mis_rate=lapply(folds,misclassification)


#Average of the missclasification
cv_error_prunnedT=mean(as.numeric(mis_rate))
paste("Misclassification Rate for prunned Tree:", round(cv_error_prunnedT*100, 2), "%")
```

**Table 22.** 
*Summary of Classification Models: Misclassification Rates and Included Predictors*

```{r, echo=FALSE}
model_comparison <- data.frame(
  Model = c("Logistic Regression", "Linear Discriminant Analysis", 
            "Quadratic Discriminant Analysis", "Classification Tree"),
  `Misclassification Rate` = c("9.00%", "9.1%", "11.15%", "9.86%"),
  `Number of Variables / Nodes` = c("9", "7", "6", "7 nodes")
  )


kable(model_comparison, format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  row_spec(0, bold = TRUE) %>%
  row_spec(1, background = "#4FC3F7") %>%
  row_spec(2, background = "#FF69B4") %>%
  row_spec(3, background = "#FFF176") %>%
  row_spec(4, background = "#00C853")
```


Based on the $10$-fold cross-validation results, Logistic Regression (GLM) emerged as the model with the lowest misclassification rate ($9.00$%), followed closely by Linear Discriminant Analysis (LDA) with $9.1$%. These findings suggest that both models offer strong predictive performance, with GLM slightly outperforming in terms of classification accuracy.

However, model evaluation should not rely solely on misclassification rates. Other performance metrics—such as precision, sensitivity, and specificity—provide a more nuanced understanding of each model's strengths and limitations, particularly in the context of marketing campaigns where the cost of false positives and false negatives can differ substantially.

The second table summarizes the classification metrics for each model before applying cross-validation:

**Table 23.** 
*Classification Performance Metrics Before Cross-Validation*
```{r, echo=FALSE}
performance_table <- data.frame(
  Model = c("GLM", "LDA", "QDA", "Classification Tree"),
  Assumptions = c("Met", "Not met", "Not met", "Met"),
  Accuracy = c("90.7%", "90.6%", "88.9%", "89.8%"),
  MR = c("9.3%", "9.4%", "11.1%", "10.2%"),
  `Precision (Yes)` = c("55.1%", "41.0%", "44.0%", "47.4%"),
  `Sensitivity (Yes)` = c("28.9%", "52.9%", "47.9%", "32.2%"),
  `Specificity (No)` = c("97.4%", "93.7%", "93.4%", "92.8%")
)

kable(performance_table, format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  row_spec(0, bold = TRUE) %>%
  row_spec(1, background = "#4FC3F7") %>%
  row_spec(2, background = "#FF69B4") %>%
  row_spec(3, background = "#FFF176") %>%
  row_spec(4, background = "#00C853")


```


GLM is recommended if the objective is to minimize false positives and improve overall prediction accuracy. In contrast, LDA may be more suitable when the marketing strategy emphasizes maximizing subscriber detection, accepting a higher false positive rate as a trade-off.

Ultimately, the optimal model selection depends on the specific goals and resource constraints of the campaign. These performance results provide a robust basis for aligning statistical accuracy with operational effectiveness.


# Conclusion 


Demographic attributes showed limited influence on subscription behavior. While only education was significant in the logistic regression model, both age and education were included in the LDA and QDA models, suggesting these variables may contribute under certain modeling assumptions, but their overall impact appears modest.

Among economic indicators, the employment variation rate was the only variable retained, and even then, only in one model. This suggests that macroeconomic fluctuations have limited direct impact on individual subscription decisions within the context of this campaign.

In contrast, campaign-related variables such as the month of contact and call duration were consistently identified as important across all models, GLM, LDA, and the classification tree. This consistency underscores the operational importance of timing and customer engagement length as key drivers of campaign effectiveness.

The outcome of previous campaigns (‘poutcome’) was only considered in the LDA model and excluded from the others, indicating that past campaign results have limited predictive value in this context and may not be a key factor influencing current customer decisions.

Ultimately, the choice of the “best” model depends on the marketing campaign’s strategic priorities. While the GLM model achieved the lowest misclassification rate ($9.00$%), followed closely by LDA ($9.10$%). Other performance metrics, such as sensitivity, precision, and specificity must also be considered. For instance, LDA exhibited higher sensitivity ($52.9$%) compared to GLM ($28.9$%), making it more suitable in scenarios where correctly identifying potential subscribers (true positives) is a priority.

Therefore, model selection should be aligned with the specific objectives of the marketing strategy. If minimizing false positives is more critical, a model with higher specificity (such as GLM) may be preferred. Conversely, if capturing more true positives is essential, LDA may be the more appropriate choice despite a slightly higher misclassification rate.

\newpage

# References 

Yamahata, H. (n.d.). Bank Marketing. Kaggle. https://www.kaggle.com/datasets/henriqueyamahata/bank-marketing

























































